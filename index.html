<!DOCTYPE html>
<html>
<body>

<h1 style="color:DodgerBlue;"><large>  Data Science Portfolio by William Ma</large></h1>
<p style="font-size:20px;LINE-HEIGHT:30px;">
	The portfolio is a complilation of Jupyter Notebooks which I created for data exploration 
and machine learning analysis. Projects are listed by different categories. All projects are implemented by Python.
<br>
<br>
</p>


<h2>Regression Problem: Predict Future Sales (Kaggle competition)</h2>
<p style="font-size:20px;LINE-HEIGHT:30px;">
	This dataset is based on a time-series consisting of "daily sales data for each shop" plus "product item" combinations covering 34 months of durations. The dataset is provided by one of the largest Russian software firms ¡V 1C Company from Kaggle. In addition, different dataset with different information on each table such as shop names, item categories, and item names, etc. The goal is to predict total sales for every shop and product combinations in the next month (2015/11) evaluated by RMSE.

<br>
Github: <a href="https://github.com/weimingma/Kaggle_Predict_Future_Sales/blob/main/Predict_future_sales_main.ipynb">here</a>
<br>
<br>
</p>



<h2>Regression / Time Series Problem: Bike Sharing Demand (Kaggle competition)</h2>
<p style="font-size:20px;LINE-HEIGHT:30px;">
	This project is a competition in Kaggle with hourly bike rental data (like U-bike rental system in Taiwan) spanning two years, The first 19 days of data in each month has been provided and the target is to predict all the rental counts hourly from the 20th to last day of each month. Several machine learning models are built to compare the performance. As there are another two features (Casual, Registered) and their sum is exactly the values of Counts, two seperate predictions are made first for Casual and Registered and their sums are used to make predictions to get lower RMSLE. Gradient Boosting algorithm is used for final prediction and submission. 
<br>
Github: <a href="https://github.com/weimingma/Bike-Sharing-Demand/blob/main/Main_bike%20sharing%20demand.ipynb">here</a>
<br>
<br>
</p>

<h2>Regression Problem: House Prices Advanced Regression Techniques (Kaggle competition)</h2>
<p style="font-size:20px;LINE-HEIGHT:30px;">
	This project is a compeition in Kaggle with initially 79 features for the residential homes in Ames, Iowa. The goal of this challenge is 
to predict the final price of each home. As usual, data exploration, feature engineering are applied, then several machine learning models are built to predict 
the house price in the test set. Linear Regression and XGBoost are implemented to predict house price with hundreds of features. 
<br>
Github: <a href="https://github.com/gtset164/Kaggle_HousePrices/blob/master/house%20price.ipynb">here</a>
<br>
<br>
</p>

<h2>Classification Problem: Titanic Machine Learning from Disaster (Kaggle competition)</h2>
<p style="font-size:20px;LINE-HEIGHT:30px;">
	Titanic is a famous competition in Kaggle. Many people start machine learning projects from this Kaggle Competition, so do I. 
Although the data in Kaggle competition is usually from better structured dataset, I still think it's a very good training platform as well as
 a good place to gain data science experience on EDA, feature engineering, and machine learning by learning from so many experts around the world. 
<br>
Github: <a href="https://github.com/weimingma/Kaggle-Titanic/blob/master/Kaggle%20Titanic%20predictions.ipynb">here</a>
<br>
<br>
</p>

<h2>Multi-Classification Problem: Digit Recognizer (Kaggle competition)</h2>
<p style="font-size:20px;LINE-HEIGHT:30px;">
	Classified handwritten images 0-9 using Neural Networks and KNN algorithms.
<br>
Github: <a href="https://github.com/gtset164/Kaggle_Digit_Recognizer/blob/master/Digit%20Recognier%20main.ipynb">here</a>
<br>
<br>
</p>

<h2>Standalone Project: Boston Marathon Data Analysis </h2>
<p style="font-size:20px;LINE-HEIGHT:30px;">
	Exploratory Data Analysis is applied on 2017 Boston Marathon dataset for different ages, genders, and the time records by distance splits.
"Pace stabiliy" is calculated by the statistical indicator Coefficient of Variation to describe how stable the pace is for each runners during the whole marathon.
The result shows that good runners tend to run more stably in every periods of pace.

<br>
The original data is available from <a href="https://www.kaggle.com/rojour/boston-results">here</a>
<br>
Github: <a href="https://github.com/weimingma/Boston-Marathon/blob/master/main.ipynb">here</a>
<br>
<br>
</p>


</body>
</html>